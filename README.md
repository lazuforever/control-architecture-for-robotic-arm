# ğŸ¤– Robot AntropomÃ³rfico con Control Multimodal - ROS2

Sistema de control para robot antropomÃ³rfico de 4 grados de libertad que integra visiÃ³n artificial, comandos de voz y planificaciÃ³n de trayectorias mediante ROS2 Humble.

## ğŸ¯ DescripciÃ³n General

Este proyecto implementa una arquitectura distribuida en ROS2 para el control de un robot manipulador mediante interacciÃ³n multimodal, combinando:

- **Comandos de voz** a travÃ©s de Amazon Alexa
- **VisiÃ³n artificial** con detecciÃ³n de gestos mediante MediaPipe
- **PlanificaciÃ³n de trayectorias** usando MoveIt2
- **Control de hardware** mediante interfaz personalizada ros2_control

## âœ¨ CaracterÃ­sticas Principales

- âœ… Interfaz de hardware personalizada para comunicaciÃ³n serial con motores Dynamixel
- âœ… DetecciÃ³n de posiciÃ³n de mano en 3D usando MediaPipe Hands
- âœ… IntegraciÃ³n con Alexa Skills Kit para control por voz
- âœ… PlanificaciÃ³n automÃ¡tica de trayectorias con MoveIt2 y OMPL
- âœ… VisualizaciÃ³n en tiempo real en RViz2
- âœ… Operaciones Pick & Place mediante gestos  

## ğŸ—ï¸ Arquitectura del Sistema

```mermaid
graph TD
    A[INTERACCIÃ“N MULTIMODAL] --> B[Alexa Interface<br/>Flask + ASK]
    A --> C[Finger Detector<br/>MediaPipe + OpenCV]
    
    B --> D[Task Server C++<br/>Action Server ROS2]
    C --> D
    
    D --> E[MoveIt2<br/>Planning]
    E --> F[ros2_control<br/>Controller Manager]
    F --> G[Rapling Interface<br/>Hardware]
    G --> H[ESP32 +<br/>Dynamixel AX-12A]
    
    style A fill:#e1f5ff
    style D fill:#fff4e1
    style E fill:#e8f5e9
    style H fill:#fce4ec
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas

### Sistema Operativo y Framework
- **Ubuntu 22.04 LTS** - Plataforma base con soporte hasta 2027
- **ROS2 Humble Hawksbill** - Framework de robÃ³tica distribuida

### Control y PlanificaciÃ³n
- **MoveIt2** - PlanificaciÃ³n de trayectorias y cinemÃ¡tica inversa
- **ros2_control** - Framework de control de hardware
- **OMPL** - LibrerÃ­a de planificaciÃ³n de movimientos
- **Plugin personalizado** - Interfaz de hardware para comunicaciÃ³n serial

### VisiÃ³n Artificial
- **MediaPipe Hands** - DetecciÃ³n de 21 landmarks de mano en tiempo real
- **OpenCV** - Procesamiento de imagen y calibraciÃ³n de cÃ¡mara
- **TF2** - Transformaciones entre sistemas de coordenadas

### InteracciÃ³n de Voz
- **Amazon Alexa Skills Kit** - Procesamiento de comandos de voz
- **Flask** - Servidor web para comunicaciÃ³n con Alexa
- **ngrok** - TÃºnel HTTPS para desarrollo local

### Hardware
- **ESP32** - Microcontrolador dual-core para gestiÃ³n de motores
- **Dynamixel AX-12A** - Servomotores inteligentes con retroalimentaciÃ³n
- **Buffer 74LS241** - AdaptaciÃ³n de seÃ±al half-duplex TTL
- **CÃ¡mara Logitech C920s** - Sensor RGB para visiÃ³n artificial

## ğŸ“¦ Estructura de Paquetes

```
src/
â”œâ”€â”€ rapling_description/       # Modelo URDF del robot
â”‚   â”œâ”€â”€ urdf/                  # Archivos Xacro con geometrÃ­a
â”‚   â”œâ”€â”€ meshes/                # Modelos 3D (STL)
â”‚   â””â”€â”€ launch/                # VisualizaciÃ³n en RViz
â”‚
â”œâ”€â”€ rapling_controller/        # Control de hardware
â”‚   â”œâ”€â”€ include/               # Plugin de interfaz (HPP)
â”‚   â”œâ”€â”€ src/                   # ImplementaciÃ³n serial (CPP)
â”‚   â””â”€â”€ config/                # Controladores y lÃ­mites
â”‚
â”œâ”€â”€ rapling_moveit_2/          # ConfiguraciÃ³n MoveIt2
â”‚   â”œâ”€â”€ config/                # SRDF, cinemÃ¡tica, lÃ­mites
â”‚   â””â”€â”€ launch/                # Move group + RViz
â”‚
â”œâ”€â”€ rapling_remote/            # IntegraciÃ³n multimodal
â”‚   â”œâ”€â”€ alexa_interface.py     # Servidor Flask + Alexa
â”‚   â””â”€â”€ task_server.cpp        # Action server de tareas
â”‚
â”œâ”€â”€ learning_topic/            # VisiÃ³n artificial
â”‚   â”œâ”€â”€ finger_detector.py     # DetecciÃ³n con MediaPipe
â”‚   â””â”€â”€ topic_webcam_pub.py    # Publicador de imagen
â”‚
â””â”€â”€ rapling_msgs/              # Mensajes personalizados
    â””â”€â”€ action/                # DefiniciÃ³n de acciones
```

## ğŸ”§ ImplementaciÃ³n TÃ©cnica

### 1. Modelo Digital del Robot
- ExportaciÃ³n desde SolidWorks usando plugin URDF Exporter
- DefiniciÃ³n de 4 articulaciones rotacionales con lÃ­mites
- IntegraciÃ³n con ros2_control mediante tags Xacro
- CalibraciÃ³n de offsets entre cero simulado y cero fÃ­sico

### 2. Interfaz de Hardware (rapling_controller)
- Plugin C++ basado en `hardware_interface::SystemInterface`
- ComunicaciÃ³n serial a 115200 baudios con ESP32
- ExportaciÃ³n de `StateInterface` y `CommandInterface` por articulaciÃ³n
- ConversiÃ³n automÃ¡tica radianes â†” grados con compensaciÃ³n de offsets
- PublicaciÃ³n de `JointState` para retroalimentaciÃ³n en tiempo real

### 3. Sistema de VisiÃ³n (finger_detector)
- Captura de frames desde `/image_raw` (10 Hz)
- DetecciÃ³n de landmark 8 (punta del Ã­ndice) con MediaPipe
- ConversiÃ³n pÃ­xel â†’ coordenadas 3D usando parÃ¡metros intrÃ­nsecos
- TransformaciÃ³n al frame `world` mediante TF2
- DetecciÃ³n de gesto: mano cerrada + posiciÃ³n estable (3s) = punto guardado
- PublicaciÃ³n de marcadores como `visualization_msgs/MarkerArray`

### 4. IntegraciÃ³n con Alexa (alexa_interface.py)
- Servidor Flask en puerto 5000 expuesto mediante ngrok
- Handlers para intents: `LaunchRequest`, `PickIntent`, `WakeIntent`, `SleepIntent`
- SuscripciÃ³n al tÃ³pico `/finger_poses` para obtener coordenadas
- EnvÃ­o de goals al Action Server usando `rclpy.action.ActionClient`
- EjecuciÃ³n en hilo separado para compatibilidad Flask + ROS2 spin

### 5. Servidor de Tareas (task_server.cpp)
- Action Server basado en `rclcpp_action`
- RecepciÃ³n de goals con campo `task_number` (0-9)
- Interfaz con MoveIt2 mediante `MoveGroupInterface`
- PlanificaciÃ³n con OMPL y parametrizaciÃ³n temporal IPTP
- EjecuciÃ³n mediante `arm_controller` (JointTrajectoryController)

### 6. ComunicaciÃ³n Hardware
- ESP32 con arquitectura dual-core:
  - **Core 0**: Lectura de encoders + envÃ­o de estados
  - **Core 1**: Procesamiento de comandos + control de motores
- Protocolo Dynamixel con daisy-chain en bus compartido
- Control de direcciÃ³n half-duplex mediante GPIO4
- CompensaciÃ³n de motores acoplados (M2A/M2B en hombro)

## ğŸš€ Flujo de OperaciÃ³n

### ConfiguraciÃ³n Inicial

#### 1. ConexiÃ³n del Hardware
```bash
# Verificar puerto serial disponible
ls /dev/ttyUSB*

# Si el puerto es diferente a /dev/ttyUSB0, modificar en:
# rapling_description/urdf/rapling_ros2_control.xacro
# <param name="port">/dev/ttyUSB0</param>
```

#### 2. Lanzar el Sistema Completo
```bash
# Inicia todos los nodos: control, MoveIt2, visiÃ³n y Alexa
ros2 launch rapling_bringup real_robot.launch.py
```

> **âš ï¸ Importante:** Al iniciar, el robot ejecuta una rutina de **sincronizaciÃ³n automÃ¡tica**:
> - Lee la posiciÃ³n actual de los motores
> - Mueve todas las articulaciones a la posiciÃ³n **HOME**
> - Si no alcanza la posiciÃ³n ideal, el programa no arrancarÃ¡

#### 3. Configurar ngrok para Alexa
```bash
# Ejecutar ngrok para crear tÃºnel HTTPS
ngrok http 5000

# Copiar la URL generada (ej: https://xxxx.ngrok.io)
# Configurarla en Alexa Developer Console como endpoint
```

**Recursos de configuraciÃ³n:**
- [Tutorial ngrok + Alexa parte 1](https://www.youtube.com/watch?v=dgKL519EF4Q&list=PL9R2s5XMUJUMrBar9WkCkY-oBbvG3DltF&index=76)
- [Tutorial ngrok + Alexa parte 2](https://www.youtube.com/watch?v=AkZKx2bMaQM&list=PL9R2s5XMUJUMrBar9WkCkY-oBbvG3DltF&index=77)

---

### OperaciÃ³n: Pick & Place con Gestos

#### Paso 1: Capturar Punto de Origen (Pick)
1. Coloque su **dedo Ã­ndice** en la posiciÃ³n deseada
2. El sistema detecta automÃ¡ticamente la punta del Ã­ndice con MediaPipe
3. **Cierre el puÃ±o durante 3 segundos** para guardar el punto
4. AparecerÃ¡ una **esfera verde** en RViz marcando la posiciÃ³n

#### Paso 2: Capturar Punto de Destino (Place)
1. Mueva su mano a la segunda posiciÃ³n
2. Repita el gesto: **cierre el puÃ±o 3 segundos**
3. AparecerÃ¡ una segunda esfera verde en RViz

> **ğŸ’¡ Reiniciar puntos:** Abra completamente la mano para que el sistema vuelva a detectar el primer punto

#### Paso 3: Ejecutar con Comandos de Voz

**OpciÃ³n A - Pick & Place:**
```
Usuario: "Alexa, Control Robot"          â†’ Activa la Skill
Usuario: "Alexa, Ejecutar movimiento"    â†’ Inicia secuencia Pickâ†’Homeâ†’Place
```

**OpciÃ³n B - Unir Puntos:**
```
Usuario: "Alexa, Control Robot"          â†’ Activa la Skill
Usuario: "Alexa, Unir puntos"            â†’ Robot traza lÃ­nea entre puntos
```

#### Paso 4: EjecuciÃ³n AutomÃ¡tica
```
Alexa â†’ Flask â†’ Task Server â†’ MoveIt2 planifica trayectoria
     â†’ OMPL genera waypoints articulares
     â†’ IPTP parametriza velocidades/aceleraciones
     â†’ arm_controller ejecuta la trayectoria
     â†’ Rapling Interface convierte radâ†’grados+offset
     â†’ ESP32 envÃ­a comandos seriales a Dynamixels
     â†’ Motores ejecutan el movimiento
```

## ğŸ“Š Resultados

- âœ… ConversiÃ³n precisa entre cero simulado y fÃ­sico de motores
- âœ… DetecciÃ³n estable de mano con precisiÃ³n < 30px de desviaciÃ³n
- âœ… Trayectorias suaves con perfiles trapezoidales de velocidad
- âœ… Tiempo de respuesta < 2s desde comando de voz hasta inicio de movimiento
- âœ… SincronizaciÃ³n exitosa entre planificaciÃ³n y hardware real  

## ğŸ’» Requisitos

- **Ubuntu 22.04 LTS**
- **ROS2 Humble**
- **Python 3.10+** con: `opencv-python`, `mediapipe`, `flask`, `ask-sdk-core`
- **Hardware**: ESP32, 4x Dynamixel AX-12A, CÃ¡mara USB, Alexa device

## ğŸ“– Uso RÃ¡pido

```bash
# VisualizaciÃ³n del modelo
ros2 launch rapling_description display.launch.py

# Sistema completo
ros2 launch rapling_controller controller.launch.py
ros2 launch rapling_moveit_2 moveit.launch.py
ros2 launch learning_topic vision_launch.py
python3 src/rapling_remote/alexa_interface.py
```

## ğŸ“§ Contacto

**Brayan Macana**

- ğŸ“§ Email: brayandayani@hotmail.com
- ğŸ“± TelÃ©fono: +57 312 364 4501
- ğŸ“ UbicaciÃ³n: Colombia

Para consultas, sugerencias o colaboraciones sobre el proyecto, no dudes en contactarme.

---

<p align="center">
  Desarrollado con ROS2 Humble | MoveIt2 | MediaPipe | Alexa Skills Kit
</p>
